{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f6be5c",
   "metadata": {},
   "source": [
    "## Experiments with STFT preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787eada",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8e6d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D, Conv2D\n",
    "from keras.layers import MaxPooling1D, MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5c60d",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4127a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label resolution of classification\n",
    "RESOLUTION = 1\n",
    "\n",
    "# Number of samples to include while creating one ML feature\n",
    "SAMPLES = 2048\n",
    "\n",
    "# Determines the overlap of samples between consecutive features\n",
    "STEP = 1024\n",
    "\n",
    "AUDIO_PATH = '../training_data/audio'\n",
    "\n",
    "# Number of microphones on the array\n",
    "MICS_NUMBER = 6\n",
    "\n",
    "MIC_COMBS = len(list(combinations(range(MICS_NUMBER), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658fa04",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3babeff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_observations(wav_signals, fs, label, samples=1, step=1, resolution=20, music=False):\n",
    "    \"\"\"\n",
    "    Create list of observations from the pandas dataframe.\n",
    "    Each observation will be a STFT matrix, where each row \n",
    "    is a vector of STFT for a given microphone.\n",
    "    \n",
    "    Returns: \n",
    "        a tuple of observations and their corresponding labels\n",
    "    \"\"\"\n",
    "    rounded_label = round(label / resolution) * resolution\n",
    "    if rounded_label == 360: rounded_label = 0\n",
    "        \n",
    "    X = stft.analysis(wav_signals, L=SAMPLES, hop=STEP)\n",
    "    X = np.transpose(X, axes=[0, 2, 1])\n",
    "    y = [rounded_label] * len(X)\n",
    "    \n",
    "    return np.angle(X), y\n",
    "\n",
    "\n",
    "def create_dataframe(subset, plane='horizontal', samples=20, step=5, resolution=20, is_info=True):\n",
    "    \"\"\"\n",
    "    Creates a whole dataframe \n",
    "    It is achieved by looping through all WAV files in the directory\n",
    "    and creating observations from each of them. \n",
    "    \n",
    "    These observations are then all concatenated together \n",
    "    into one large dataframe\n",
    "    \n",
    "    Returns:\n",
    "        a pandas dataframe containing all data points (without any splits)\n",
    "    \"\"\"\n",
    "    \n",
    "    files = [file for file in os.listdir(os.path.join(AUDIO_PATH, plane)) if subset in file]\n",
    "    rows = 0\n",
    "\n",
    "    # Loop through all WAVs\n",
    "    for i, file in enumerate(files):\n",
    "        if file[-3:] != 'wav': \n",
    "            continue\n",
    "            \n",
    "        print(f'{subset} file {i+1}/{len(files)}', end='\\r')\n",
    "\n",
    "        path = os.path.join(AUDIO_PATH, plane, file)\n",
    "        fs, wav_signals = wavfile.read(path)\n",
    "        \n",
    "        label = int(file.split('_')[2])\n",
    "        \n",
    "        # Create observations from a given WAV file\n",
    "        X_temp, y_temp = create_observations(wav_signals, fs, label, samples, step, resolution)\n",
    "        \n",
    "        cols = [\n",
    "            f'mic{mic+1}_{i}' \n",
    "                for mic in range(MICS_NUMBER)\n",
    "                    for i in range(np.shape(X_temp)[2])\n",
    "        ] if i == 0 else None\n",
    "        \n",
    "        df = pd.DataFrame(data=np.reshape(X_temp, (len(X_temp), -1)), columns=cols)\n",
    "        \n",
    "        # Add extra info columns\n",
    "        if is_info:\n",
    "            dist = int(file.split('_')[4])\n",
    "            room = file.split('_')[6]\n",
    "            df['dist'], df['room'] = dist, room\n",
    "            \n",
    "        # Add label column\n",
    "        df['label'] = y_temp\n",
    "        rows += df.shape[0]\n",
    "        df.to_csv(f'../training_data/stft_azimuth_{subset}_dataset.csv', index=False, mode='a' if i else 'w', header=(i==0))\n",
    "        \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e629a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rows = create_dataframe('train', samples=SAMPLES, step=STEP, resolution=RESOLUTION)\n",
    "print()\n",
    "test_rows = create_dataframe('test', samples=SAMPLES, step=STEP, resolution=RESOLUTION)\n",
    "print()\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "encoder = encoder.fit([[label] for label in range(0, 360, RESOLUTION)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6520eab",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e3918",
   "metadata": {},
   "source": [
    "Implement a generator to read data from CSV in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ebd490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(file_path, batch_size):\n",
    "    df_iterator = pd.read_csv(file_path, iterator=True, chunksize=batch_size)\n",
    "    while True:\n",
    "        for df in df_iterator:\n",
    "            X = df.drop(columns=['dist', 'room', 'label']).values.reshape(batch_size, -1, MICS_NUMBER, 1, order='F')\n",
    "            y = df.label.values.reshape(-1, 1)\n",
    "            y = encoder.transform(y)\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7db118",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 6430/19845 [========>.....................] - ETA: 48:59 - loss: 6.2637 - accuracy: 0.0647"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "epochs, batch_size, verbose = 5, 32, 1\n",
    "steps_per_epoch = train_rows // batch_size\n",
    "\n",
    "n_timesteps, n_features, n_outputs = 1025, MICS_NUMBER, 360\n",
    "\n",
    "# Init model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Conv2D(filters=64, kernel_size=(1, 2), activation='relu', input_shape=(n_timesteps,n_features, 1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(1, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(1, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(1, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(1, 2), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(generate_data('../training_data/stft_azimuth_train_dataset.csv', batch_size),\n",
    "                    epochs=epochs, verbose=verbose, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7db7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(generate_data('../training_data/stft_azimuth_test_dataset.csv', batch_size), \n",
    "                                steps=test_rows // batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
