{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250e51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import math\n",
    "import pyroomacoustics as pra\n",
    "from pyroomacoustics.utilities import normalize\n",
    "from pyroomacoustics.transform import stft\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='dark', palette='muted', font_scale=1)\n",
    "rcParams['figure.figsize'] = 22, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0805181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label resolution of classification\n",
    "RESOLUTION = 10\n",
    "\n",
    "# Number of samples to include while creating one ML feature\n",
    "SAMPLES = 2048\n",
    "\n",
    "# Determines the overlap of samples between consecutive features\n",
    "STEP = 8192\n",
    "\n",
    "# Training rooms dimensions\n",
    "ROOMS = {\n",
    "    'small' : np.array([4, 4, 3]),\n",
    "    'medium' : np.array([6, 6, 3]),\n",
    "    'large' : np.array([8, 8, 3])\n",
    "}\n",
    "\n",
    "# Testing rooms dimensions\n",
    "TEST_ROOMS = {\n",
    "    'small' : np.array([5, 5, 2]),\n",
    "    'medium' : np.array([7, 7, 2]),\n",
    "    'large' : np.array([9, 9, 2])\n",
    "}\n",
    "\n",
    "AUDIO_PATH = '../training_data/audio/multi_source'\n",
    "\n",
    "# Number of microphones on the array\n",
    "MICS_NUMBER = 6\n",
    "\n",
    "MIC_COMBS = len(list(combinations(range(MICS_NUMBER), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9231ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_encode(encoder, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Creates a multi-hot encoding of categorical labels\n",
    "    provided in y_train and y_test.\n",
    "    \"\"\"\n",
    "    \n",
    "    # One-hot encode training and testing labels\n",
    "    enc = encoder.fit(y_train)\n",
    "    y_train = enc.transform(y_train)\n",
    "    y_test = enc.transform(y_test)\n",
    "    \n",
    "    return y_train, y_test\n",
    "  \n",
    "    \n",
    "def create_whole_dataset(df_train, df_test, encoder, room=None, dist=None):\n",
    "    \"\"\"\n",
    "    Creates an entire dataset by extracting values\n",
    "    from train and tests dataframes.\n",
    "    \n",
    "    One-hot encodes the labels before returning.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Can filter testing entries to only check performance\n",
    "    # for given conditions\n",
    "    if room:\n",
    "        df_test = df_test[df_test.room == room]\n",
    "    \n",
    "    # Create train/test observations\n",
    "    X_train = df_train.drop(columns=['room', 'label']).values\n",
    "    X_test = df_test.drop(columns=['room', 'label']).values\n",
    "    \n",
    "    # Create train/test labels\n",
    "    y_train, y_test = multi_hot_encode(\n",
    "        encoder, df_train['label'].values, df_test['label'].values)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_observations(wav_signals, label, samples=1, step=1, resolution=20):\n",
    "    # Lists of observations and labels that will be populated\n",
    "    X = tf.signal.frame(wav_signals.T, frame_length=samples, frame_step=step)\n",
    "    return np.transpose(X, axes=[1, 0, 2])\n",
    "\n",
    "\n",
    "def create_dataframe(subset, samples=20, step=5, resolution=20, is_info=True, interp=1):\n",
    "    \"\"\"\n",
    "    Creates a whole dataframe \n",
    "    It is achieved by looping through all WAV files in the directory\n",
    "    and creating observations from each of them. \n",
    "    \n",
    "    These observations are then all concatenated together \n",
    "    into one large dataframe\n",
    "    \n",
    "    Returns:\n",
    "        a pandas dataframe containing all data points (without any splits)\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    files = [file for file in os.listdir(AUDIO_PATH) if subset in file]\n",
    "\n",
    "    # Loop through all WAVs\n",
    "    for i, file in enumerate(files):\n",
    "        if file[-3:] != 'wav': \n",
    "            continue\n",
    "            \n",
    "        print(f'{subset} file {i+1}/{len(files)}', end='\\r')\n",
    "\n",
    "        path = os.path.join(AUDIO_PATH, file)\n",
    "        fs, wav_signals = wavfile.read(path)\n",
    "        \n",
    "        labels = (int(file.split('_')[2]), )\n",
    "        if file.split('_')[1] == 'angles':\n",
    "            labels = (int(file.split('_')[2]), int(file.split('_')[3]))\n",
    "            \n",
    "        X_temp = create_observations(wav_signals, labels, samples, step, resolution)\n",
    "        \n",
    "        cols = [f'mic{mic+1}_sample_{i}' for mic in range(MICS_NUMBER) for i in range(np.shape(X_temp)[2])]\n",
    "        \n",
    "        df = pd.DataFrame(data=np.reshape(X_temp, (len(X_temp), -1)), columns=cols)\n",
    "        \n",
    "        # Add extra info columns\n",
    "        if is_info:\n",
    "            room = file.split('_')[5 if file.split('_')[1] == 'angles' else 4]\n",
    "            df['room'] = room\n",
    "            \n",
    "        # Add label column\n",
    "        df['label'] = [labels] * len(df)\n",
    "        dataframes.append(df)\n",
    "        \n",
    "    return pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4bb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train file 1451/1998\r"
     ]
    }
   ],
   "source": [
    "df_train = create_dataframe('train', samples=SAMPLES, step=STEP, resolution=RESOLUTION, interp=2)\n",
    "print()\n",
    "df_test = create_dataframe('test', samples=SAMPLES, step=STEP, resolution=RESOLUTION, interp=2)\n",
    "print()\n",
    "\n",
    "encoder = MultiLabelBinarizer()\n",
    "X_train, y_train, X_test, y_test = create_whole_dataset(df_train, df_test, encoder)\n",
    "\n",
    "print(np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))\n",
    "pd.set_option('display.max_columns', 8)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e5957f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "def custom_activation(x, axis=-1):\n",
    "    return tfa.seq2seq.hardmax(x)*x\n",
    "\n",
    "\n",
    "def deepmp(input_shape,SenMat,k):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    r = inputs\n",
    "    get_custom_objects().update({'custom_activation': custom_activation})\n",
    "\n",
    "    for kk in range(k):\n",
    "        if kk == 0:\n",
    "            denf1 = Dense(\n",
    "                SenMat.shape[1],\n",
    "                activation='custom_activation',\n",
    "                trainable=True,\n",
    "                use_bias=False,\n",
    "                weights=[SenMat]\n",
    "            )\n",
    "            \n",
    "            denb1 = Dense(\n",
    "                SenMat.shape[0],\n",
    "                trainable=False,\n",
    "                use_bias=False,\n",
    "                weights=[SenMat.transpose()]\n",
    "            )\n",
    "            \n",
    "            x = denf1(r)\n",
    "            rx = denb1(x)\n",
    "            r = keras.layers.subtract([r, rx])\n",
    "            z = x\n",
    "        else:\n",
    "            denf1 = Dense(\n",
    "                SenMat.shape[1],\n",
    "                activation='custom_activation',\n",
    "                trainable=True,\n",
    "                use_bias=False,\n",
    "                weights=[SenMat]\n",
    "            )\n",
    "            \n",
    "            x = denf1(r)\n",
    "            z = keras.layers.add([z,x])\n",
    "            rx = denb1(x)\n",
    "            r = keras.layers.subtract([r, rx])\n",
    "            \n",
    "    output = z    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4eac3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 6 Output: 11\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 11)           66          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 6)            66          dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_10 (Subtract)          (None, 6)            0           input_6[0][0]                    \n",
      "                                                                 dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 11)           66          subtract_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 11)           0           dense_15[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 198\n",
      "Trainable params: 132\n",
      "Non-trainable params: 66\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "k = 2  # sparsity of the signal\n",
    "m = X_train.shape[1]\n",
    "n = y_train.shape[1]\n",
    "\n",
    "print(f'Input: {m}', f'Output: {n}')\n",
    "\n",
    "M = np.array(np.abs(np.random.standard_normal((m, n))), dtype='float32')\n",
    "\n",
    "# Normalize the dictionary as implied by the standard procedure for Matching Pursuit Algorithms.\n",
    "for ii in range(0, M.shape[1]):\n",
    "    mind = M[:, ii] ** 2\n",
    "    no = mind.sum()\n",
    "    M[:, ii] = M[:, ii] / np.sqrt(no)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "model = deepmp(input_shape=input_shape, SenMat=M, k=k)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
